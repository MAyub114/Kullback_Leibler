{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kullback-Leibler divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_KL_divergence(P, Q):\n",
    "    if len(P) != len(Q):\n",
    "        return \"ERROR: probability distributions P and Q are not equal in length.\"\n",
    "\n",
    "    divergence = 0.0\n",
    "    for x in range(len(P)):\n",
    "        divergence += P[x] * np.log(P[x] / Q[x])\n",
    "    return divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability distributions\n",
    "\n",
    "* Create distributions P (binomial) and Q (uniform)\n",
    "* Both distributions have discrete values 0, 1, and 2\n",
    "* Store values in a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>Q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.357</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.479</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.164</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       P      Q\n",
       "0  0.357  0.333\n",
       "1  0.479  0.333\n",
       "2  0.164  0.333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create binomial distribution\n",
    "decimal_points = 3\n",
    "N_trials = 10000\n",
    "distribution_binom = binom.rvs(n=2, p=0.4, size=N_trials)\n",
    "\n",
    "unique, counts = np.unique(distribution_binom, return_counts=True)\n",
    "P = pd.DataFrame(np.round(counts / N_trials, decimal_points))\n",
    "\n",
    "# Uniform distribution\n",
    "Q = pd.DataFrame(np.round([1/3, 1/3, 1/3], decimal_points))\n",
    "\n",
    "# Store in a DataFrame\n",
    "df = pd.concat([P, Q], axis=1)\n",
    "df.columns = ['P', 'Q']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence (P||Q) = 0.082832\n",
      "KL divergence (Q||P) = 0.091617\n"
     ]
    }
   ],
   "source": [
    "# Using own implementation of KL Divergence\n",
    "print(f\"KL divergence (P||Q) = {round(calculate_KL_divergence(df.P, df.Q), 6)}\")\n",
    "print(f\"KL divergence (Q||P) = {round(calculate_KL_divergence(df.Q, df.P), 6)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL divergence (P||Q) = 0.082832\n",
      "KL divergence (Q||P) = 0.091617\n"
     ]
    }
   ],
   "source": [
    "# NumPy version of KL divergence\n",
    "print(f\"KL divergence (P||Q) = {round(np.sum(sp.special.rel_entr(df.P, df.Q)), 6)}\")\n",
    "print(f\"KL divergence (Q||P) = {round(np.sum(sp.special.rel_entr(df.Q, df.P)), 6)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e62e41617f0e8a6e31cb2897d2b20e34bc661183d6c13dc1da307e94ce7df895"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
